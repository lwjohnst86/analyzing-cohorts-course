```{r setup}
source("preamble.R")
```

# Chapter 3: Running the statistical techniques

**Objectives:**

- Learn to run some common statistic techniques on different datasets and research
questions
- Learn how to extract the relevant results from these stats methods
- Understand why its important to not use or even look at p-values in the output
- Know how to understand and interpret the results (we'll get to knowing what
exactly is most useful to present and show for higher impact)
- Importance of examining multiple levels of adjustment (unadjusted, minimally,
fully), and how this can inform what is going on in the relationship.
- Knowing how to choose/decide on what variables to adjust for

- Sources of bias (not at this stage) TODO: move to analysis or wrangling chapter?

## Lesson 1: Common statistical techniques used for analyzing cohorts

*Content*:

- Common statistical techniques:
    - mixed effects modeling
    - GEE (?)
    - cox proportional hazard models
    - linear regression
    - logistic regression
    - Poisson regression
    - Conditional logistic regression (for nested case-cohort designs)

*Exercises*:

- NE: Change variables scaling or transform them to see how the estimates change...
what does that mean for interpretation? (or lesson 4)

### Exercise: Type of statistical method is important, especially for longitudinal studies

Before the development of mixed effects modeling, analyzing longitudinal data
was fairly difficult. This was because the early methods, which typically were 
some type of ANOVA or simple regression, had an assumption of independence in the
data. But longitudinal data is not independent... A single individual can have 
multiple measures done over time and are thus dependent.

Take a look at the difference between using a logistic regression model compared
to a mixed effect model.

```{r logreg_mixed}
# TODO: Check to confirm exponentiating of output.
logistic_model <- glm(prevchd ~ totchol + period, data = framingham, family = binomial)
summary(logistic_model)$coef

# show this in lesson 4?
# tidy(logistic_model, conf.int = TRUE, exponentiate = TRUE) %>% 
#     select(term, estimate, conf.low, conf.high)

# TODO: This doesn't seem to converge, so need to look into it.
mixed_model <- glmer(prevchd ~ totchol + period + (1 | randid), data = framingham, family = binomial)
summary(mixed_model)$coef

# show this in lesson 4?
# tidy(mixed_model, conf.int = TRUE) %>% 
#     select(term, estimate, conf.low, conf.high)
```


## Lesson 2: Adjustment, confounding, and modelling.

*Exercises*:

- MCQ: Which options would be most appropriate for using various model selection techniques?
    - list different conditions with possible methods. {{I can't remember what I meant here..}}
- NE: Cross-validation of a model.
- MCQ/text: Describe different impact of adjusting, etc. including collider variables.
How does that change the interpretation? How can this influence the health impacts
if inappropriately published? (or in lesson 4)
- MCQ/text: What does it mean to adjust? (how does it change interpretation)

## Lesson 3

*Exercises*:

- NE: Maybe exercise where some observations are removed and show how that impacts
the p-value...? to highlight how unreliable it is?

## Lesson 4


# Notes:





