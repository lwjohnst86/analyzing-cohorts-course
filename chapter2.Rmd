```{r setup}
source("preamble.R")
```

# Chapter 2: Data exploring, wrangling, and formatting

**Objectives**

- How to get data into form for quick exploration
- How to wrangle the data to get into a form to use for the analysis/statistic
- Know the various tools to transform the data and when is more appropriate to
use them
    - Know to be very careful of ever converting continuous into discrete
    - Know when and why to transform variables (good reasons and bad)
    - Reducing number of categories in categorical variable
- Dealing with outliers (or not)

## Lesson 1: Pre-wrangling exploration

*Content*
- Exploring data (dot/boxplots, violinplots, correlation heatmap)
    - Need to understand the data before doing any serious wrangling
TODO: Add more content, focus it on cohorts.

### Exercise: Plot individual (or summary) change over time in longitudinal datasets

Create a line plot (or summary line plot if data too big) by individual.

```{r}
framing
```

### Exercise: Visually examine the outcomes with the exposures

Create a simple visual comparing the outcome with the exposures.

```{r}
framingham %>% 
    select(time, cvd, totchol, age, bmi) %>% 
    gather(Variable, Value, -time, -cvd) %>% 
    mutate(cvd = as.factor(cvd)) %>% 
    ggplot(aes(x = Value, group = cvd, fill = cvd)) +
    geom_density(alpha = 0.6) +
    facet_grid(~ Variable, scales = "free")
```

### Exercise: Inspect graphically all variables of interest individually

Create a box and jitter plot of all the variables of interest, at the baseline visit:

```{r ex_eda}
fh_long <- framingham %>% 
    filter(time == 0) %>% 
    select(cvd, totchol, bmi, age) %>% 
    gather(Variable, Value)
    
fh_long

ggplot(fh_long, aes(x = Value)) +
    geom_histogram() +
    facet_grid(~ Variable, scales = "free")
```

## Lesson 2: Tidy cohort data and wrangling into analyzable form.

*Content*
- Tidy data, types of structure for type of analysis
- Longitudinal data in wide format
    - converting into long for use by statistical techniques (e.g. mixed effects)
    - talk about tidy data
TODO: More content, specific to cohorts.

*Exercises*
... maybe two waves that need to be combined?

### Exercise: Which are untidy data?

Identify tidy datasets given the statistical technique/study design
(or which are wrong)

- wrong ones (possibles):
    - wide instead of long for longitudinal study
    - wide instead of long for those with the disease and those without
    - different character strings for same disease (rather than numbers), e.g.
    "t2dm" and "dm" for diabetes status
    - "excel" style of two datasets in one spreadsheet

### Exercise: Long vs wide, or which side is which?

Sometimes analyses are more easily done with data all completely in a long
form, either for plotting or for summarizing.

```{r}
# TODO: Write up code for this.
```

### Exercise: Data could be tidy, but not for analysis

We have a longitudinal cohort that has some variables {{finish}}

```{r}

```

## Lesson 3: Transforming and modifying variables (maybe into two lessons?)

*Content*
- Types of transformations and why to use them:
    - Log: if data is highly skewed, if interested in output after
    backtransforming of percent impact rather than on original unit impact
    (often used)
    - square: ... (rarely done)
    - scaled: Put variables on same scale/unit (all become SD, fairly common)
    - Transformation can be used to correct "violations in regression
    assumptions", but this is not often important or necessary.
    - Use transformations such as log if the variable is unitless, or there is
    disagreement between studies on the specific unit, or if the unit value
    varies between studies. That way the result is interpreted in a way that 
    can be compared to previous and future studies.
    - Don't convert continuous to discrete. There could be non-linear relationships
- Categorical variable modication:
    - sometimes some categories are too small, so sometimes for model
    interpretation and generalizability, grouping categories makes sense,
    and to also balance the sample between groups.

*Exercise*
- NE: Here's a research question and a dataset. transform the data to be able to 
get the results to be interpreted this specific way.
- MCQ/text: Which transformation is best suited for the research question.
- NE: Here is the output. Get the data to be in this form.

### Exercise: Wide to long for longitudinal data

Depending on the specific data and how a research team decided to enter the data
into the database, ... {{complete}}

TODO: I'll need to wrangle one of the datasets to fit this form for this exercise.
Some cohort studies do this, but many don't. Still important to know how to do.

```{r}
dataname %>% 
    gather(Measure, Value, -SubjectID) %>%
    print() %>% # to show what it looks like
    separate("Measure", into = c("Measure", "Time"), sep = "_") %>%  
    # _ is one of many separations.
    print() %>% # show what it looks like
    spread(Measure, Value)
    
```

### Exercise: Reduce number of categories in these datasets

Sometimes, categorical (factor or character) variables have many levels, but only
a few observations in each level. For analyses, this is less than ideal, so it
is often useful to reduce the total number of categories by merging levels together.
This can be especially useful if interpretation of only one level is desired
compared to other levels.

Reduce the levels of education by using the `fct_recode` function from the 
`forcats` package.

```{r}
library(forcats)
fh_educ <- framingham %>% 
    mutate(educ_reduced = fct_recode(
        educ, 
        "Post-Secondary" = "College",
        "Post-Secondary" = "Vocational"
        ))

# Compare the original education variable with the reduced one
fct_count(fh_educ$educ)
fct_count(fh_educ$educ_reduced)
# TODO: Add another dataset to reduce categories e.g. PROMISE and ethnicity
```

### Exercise: Comparison between different transformations

{{Multiple part question?}}

Using different types of transformations is dependent on the specific research
question and how the data looks. It's useful to understand what each types of 
transformations do to the available data.

Explore the transforms visually.

```{r}
# TODO: show a cohort with continuous outcome, and log that
fh_transformed <- framingham %>% 
    mutate(scale_bmi = as.numeric(scale(bmi)),
           log_bmi = log(bmi),
           log10_bmi = log10(bmi),
           exp_bmi = bmi^2)

fh_transformed %>% 
    select(contains("bmi")) %>% 
    gather(bmi_variable, bmi_value) %>% 
    ggplot(aes(x = bmi_value)) +
    geom_histogram() +
    facet_grid( ~ bmi_variable, scale = "free")
```

Part 2:

- Compare the distributions between the normal bmi and scaled bmi. What is different? WHy might scaling be useful?
- Compare the distributions between the normal bmi and the log bmi. What does logging do and why might it be useful?

# Notes:

- Look over data to find problems or fit to needs of research question
- Wrangling data to get into proper for statistics
- Transforming for statistics if necessary
