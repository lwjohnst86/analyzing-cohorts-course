```{r setup}
source("preamble.R")
```

# Chapter 3: Running the statistical techniques

**Objectives:**

- Learn to run some common statistic techniques on different datasets and research
questions
- Learn how to extract the relevant results from these stats methods
- Understand why its important to not use or even look at p-values in the output
- Know how to understand and interpret the results (we'll get to knowing what
exactly is most useful to present and show for higher impact)
- Importance of examining multiple levels of adjustment (unadjusted, minimally,
fully), and how this can inform what is going on in the relationship.
- Knowing how to choose/decide on what variables to adjust for

## Lesson 1: Common statistical techniques used for analyzing cohorts

*Content*:

- Common statistical techniques:
    - mixed effects modeling
    - GEE (?)
    - cox proportional hazard models
    - linear regression
    - logistic regression
    - Poisson regression
    - Conditional logistic regression (for nested case-cohort designs)

*Exercises*:

- NE: Change variables scaling or transform them to see how the estimates change...
what does that mean for interpretation? (or lesson 4)

### Exercise: Type of statistical method is important, especially for longitudinal studies

Before the development of mixed effects modeling, analyzing longitudinal data
was fairly difficult. This was because the early methods, which typically were 
some type of ANOVA or simple regression, had an assumption of independence in the
data. But longitudinal data is not independent... A single individual can have 
multiple measures done over time and are thus dependent.

Take a look at the difference between using a logistic regression model compared
to a mixed effect model.

```{r logreg_mixed}
# TODO: Check to confirm exponentiating of output.
logistic_model <- glm(prevchd ~ totchol + period, data = framingham, family = binomial)
summary(logistic_model)$coef

# show this in lesson 4?
# tidy(logistic_model, conf.int = TRUE, exponentiate = TRUE) %>% 
#     select(term, estimate, conf.low, conf.high)

# TODO: This doesn't seem to converge, so need to look into it.
mixed_model <- glmer(prevchd ~ totchol + period + (1 | randid), data = framingham, family = binomial)
summary(mixed_model)$coef

# show this in lesson 4?
# tidy(mixed_model, conf.int = TRUE) %>% 
#     select(term, estimate, conf.low, conf.high)
```

### Exercise: Common statistical methods used

{{multi-part exercise?}}

Don't go into detail too much, just "here is the code, here are the resources"
- mixed effect
- cox model
- logistic regression
- poisson regression

## Lesson 2: Adjustment, confounding, and modelling.

*Content*:

- What it means to "adjust" for confounders? What is a confounder? (probably explained 
in one of the other epi courses, so be brief here).
    - What does adjustment mean when time is included? Gets trickier. This is why
    cross-sectional analyses are simpler analytically.
    - Danger of not controlling for variables: Simpson's Paradox. Can lead to actual
    harm if analyses not thought through and properly analyzed.
    - Techniques to determine what to adjust for (DAG, literature, IC)
    - Model selection (AIC, BIC, Cross-validation{{ya?}}; when to use each)

*Exercises*:

- MCQ: Which options would be most appropriate for using various model selection techniques?
    - list different conditions with possible methods. {{I can't remember what I meant here..}}
- NE: Cross-validation of a model.
- MCQ/text: Describe different impact of adjusting, etc. including collider variables.
How does that change the interpretation? How can this influence the health impacts
if inappropriately published? (or in lesson 4)
- MCQ/text: What does it mean to adjust? (how does it change interpretation)

### Exercise: Understanding confounding pathways

{{MCQ, probably as a Bullet/TabExercise}}

- MCQ/text: Present a DAG of hypothesized variables and pathways. Which of the
following (or write out which) are the important variables to consider/adjust for.

The below diagram is a classic example of what confouding is. When a variable is 
linked in some way, either directly or indirectly, to both the exposure and the 
outcome, it is considered a confounder.

```{r simple_confounding}
grViz("
digraph {
    node [style = filled fillcolor = none]
    {
        rank = same
        Exposure [fillcolor = 'LightBlue'] 
        Outcome [fillcolor = 'OrangeRed']
    }
    Exposure -> Outcome
    Confounder -> {Exposure Outcome}
}")
```

This simple confounding example shows how each variable relates to each other in
hypothesized pathways. Understanding how variables confounded the relationship
between the exposure and the outcome is essential to drawing more accurate
inferences about the associations. Creating these DAGs of the hypothesized
pathways is a powerful tool to understanding what could be confounders, what
could be colliders, and what needs to be adjusted for. Which are more
appropriate choices:

```{r confouding_example}
grViz("
digraph {
    node [style = filled fillcolor = none]
    {
        rank = same
        BodyFat [fillcolor = 'LightBlue'] 
        CVD [fillcolor = 'OrangeRed']
    }
    BodyFat -> CVD
    Sex -> {BodyFat CVD Testosterone}
    Testosterone -> {BodyFat CVD}
}")
```

- Options: {{likely all answers will be correct.. or maybe rank them ...}}
    - Since sex influences testosterone and since both sex and testosterone
    are confounders, only need to adjust for either of these variables to control
    for the confounding pathway.
    - Adjust for both 
    - Add others {{finish}}
    
Using the pathway below, which variables, at a minimum, should you adjust for to
control maximally for potential bias in the model?

```{r confounder_complex}
grViz("
digraph {
    node [style = filled fillcolor = none]
    {
        rank = same
        BodyFat [fillcolor = 'LightBlue'] 
        CVD [fillcolor = 'OrangeRed']
    }
    BodyFat -> CVD
    Sex -> {BodyFat CVD Testosterone ExerciseType}
    Testosterone -> {BodyFat CVD}
    ExerciseType -> {BodyFat CVD}
}")
```

- Options:
    - All of them {{possible}}
    - Only sex {{posible}}
    - Only testosterone
    - Only exercise type
    - Either sex and testosterone or sex and exercise type {{possible}}
    
Consider the below graph. Which variables, at a minimum, should you adjust for?

```{r confounding_collider}
grViz("
digraph {
    node [style = filled fillcolor = none]
    {
        rank = same
        BodyFat [fillcolor = 'LightBlue'] 
        CVD [fillcolor = 'OrangeRed']
    }
    BodyFat -> CVD
    Sex -> {BodyFat CVD Testosterone ExerciseType}
    Testosterone -> {BodyFat CVD}
    ExerciseType -> {Testosterone BodyFat CVD}
}")
```

- Options:
    - All of them
    - Only sex {{possible}}
    - Only testosterone
    - Only exercise type
    - Either sex and testosterone or sex and exercise type or exercise and
    testosterone. {{definite}}
    
### Video explaining exercise above?

Go a bit into the math of why the answers for the above exercise.

### Exercise: Model selection using Information Criterion

When you are unsure of which variables may provide a better fit and maybe
explain the model and results more, using model information criterion methods
can help narrow the possible models down. For instance when deciding on
appropriate covariates to adjust for, using this technique combined with other 
techniques such as literature based or DAG based can provide more information on
what to best adjust for. Information criterion such as Akaike Information
Criterion (AIC) are used to balance good model fit with low model complexity
(i.e. less variables adjusted for). Run this command to identify which model is
the "best" of those compared.

```{r aic_select}
# TODO: Make more appropriate models (more typically seen in real analyses).
# TODO: Confirm that MuMIn is the best package for learners to use.
# TODO: Switch over to use glmer and random effects.
library(MuMIn)
model_sel_df <- framingham %>% 
    select(cvd, totchol, sex, bmi) %>% 
    na.omit()
m1 <- glm(cvd ~ totchol, data = model_sel_df, family = binomial)
m2 <- glm(cvd ~ totchol + sex, data = model_sel_df, family = binomial)
m3 <- glm(cvd ~ totchol + sex + bmi, data = model_sel_df, family = binomial)

model.sel(m1, m2, m3)

# Which is the "best" model from these three?
"m3"
```

### Exercise: Models and inappropriate adjustment

Run model without adjusting, with adjusting, and lastly with adjusting for an
inappropriate variable (how that changes things). Notice how that may change the
results. (Don't worry about what the numbers mean just yet, focus on the differences
of the estimates between models.)

```{r time_adjustment}
# TODO: Confirm that period and randid are sorted properly.
# TODO: Confirm that cvd variable is the right one (from raw data).
unadjusted <- lmer(prevchd ~ totchol + period + (1 | randid), data = framingham)
summary(unadjusted)$coef

adjusted <- lmer(prevchd ~ totchol + sex + period + (1 | randid), data = framingham)
summary(adjusted)$coef

inappropriate <- lmer(prevchd ~ totchol + sex + age + period + (1 | randid), data = framingham)
summary(inappropriate)$coef
```

### Exercise: Interpretation and explanation of previous exercise

{{Maybe a Tab/BulletExercise}}

Given the results, what are some possible conclusions/interpretations?

- Options:
    - ...

How might the adjustment for age AND period influence the results?

- Options:
    - ...

## Lesson 3: Interaction testing and sensitivity analyses

*Content*:

- Sensitivity analyses
    - exploring potential sub-hypotheses to test assumptions of models
    - e.g. stratified analysis, models with and without covars to id which is
    impacting results, etc
- Simpson's paradox? need to consider subgroup analyses (interactions), especially
for sex and ethnicity. (Expand more here, tho mentioned in lesson 2)
- Interaction testing, especially for sex and ethnicity (common requirement in order
to publish)

*Exercises*:

- NE: Maybe exercise where some observations are removed and show how that impacts
the p-value...? to highlight how unreliable it is?

### Exercise: Identifying variables that strongly influence model

### Exercise: Removing observations that strongly influence model

### Exercise: Testing for interactions of important variables

In the past (and still fairly common now), most research was done only on males.
Clinical trials, experimental animal models, and observational studies tended 
to either explicitly only study males, or to disregard the role that biological
sex had on the object of study. This had disasterous results, especially when it
came to drugs. In clinical trials, a drug appeared to work amazingly and was
passed for public use {{wording}}. Afterward, with observational studies tracking
the impact of drugs in the population, often times the drug would not work at all
or have harmful side effects in women. As a result, most journals and funding 
agencies *require* that sex and ethnicity be tested or studied.

Compare models without and with interactions for sex.

```{r sex_ethn_interaction}
no_interaction <- lmer(prevchd ~ totchol + period + (1 | randid), data = framingham)
summary(no_interaction)

sex_interaction <- lmer(prevchd ~ totchol * sex + period + (1 | randid), data = framingham)
summary(sex_interaction)

sex_time_interaction <- lmer(prevchd ~ totchol * sex * period + (1 | randid), data = framingham)
summary(sex_time_interaction)

# TODO: confirm if ethnicity is in framingham
```

## Lesson 4: Extracting relevant data from results and post-modelling-wrangling {{wording needs changes}}

*Content*:

- Using broom, what to take from broom, conf.int
- Interpreting OR, RR, IRR (incidence risk ratio), and other forms of estimates
- Transform estimates for better interpretability
- Quick overview of the unreliability of p-values, the danger of using them to
inform clinical and public health policy (find examples?)

*Exercises*:

- MCQ/text: Which are the most appropriate interpretations for OR, RR, IRR?
    
### Exercise: Cleaning up the results using the broom package

Most methods developed into R packages in R have their own unique way of showing
the results from the models, so there is no standard way to present the output of
the models. So the broom package was created to make a unified interface
(similar to the `summary` function) of the output in a "tidy" {{link}} format.

Run these code to tidy up the output, using the `tidy` function. Extract only 
relevant

```{r}
# TODO: Add family. Use glmer, but fix the "non-convergence" issue.
model <- lmer(prevchd ~ totchol + period + (1 | randid), data = framingham)

# Tidy it up
tidy(model)

# Tidy but with confidence interval (CI)
tidy_model <- tidy(model, conf.int = TRUE)
tidy_model

# Select only the important variables.
tidy_model %>% 
    select(term, estimate, conf.low, conf.high)
```

{{TODO: add another exercise expanding on this to make estimate more meaningful}}

### Exercise: "Not statistically significant" does not equal "not biologically or clinically significant"

{{MCQ; TabExercise?}}

{{This may need to be shortened and/or changed... but I want to get this point
across somehow}}

There is a very strong culture within science to focus on "statistically
significant" results, and this is no different in health research. However,
there are some major problems with this behaviour and practice not just for
science but also for health and disease outcomes.

Let's make an example: Premature babies often face severe health problems as
they grow and need substantial medical and nutritional assistance to ensure a
healthier growth. Nutrition is a key component and there are infant formula and
intravenous fluids specifically designed for premature infants. A study
observing the role of these formula on the premature babies' health found an
odds ratio of 1.12 (0.94 to 1.30 95% CI, p=0.09) of getting a health
complication from current formula for premature babies born earlier compared to
premature babies born later in gestational age. How would this be interpreted?

- There was no significant association seen (p>0.05, odds ratio passes through
the 1.0 threshold).
- There is a small, but potentially clinically important association seen (OR
had up to 1.30 in the CI).
    - {{correct. Even though it is not "statistically significant", to improve
    health outcomes in premature babies, the formula may need to change.}}
- Can't really say anything... null hypothesis was not rejected.
    - {{slightly true, but this shouldn't be focused}}

{{Next MCQ}}

Great! It's really important to understand this concept in health research, and
how you present your results changes based on what you "believe" is important.
So why is this important?

- This isn't important! The p-value was larger than 0.05!
- It is important, but we can't really say anything until more research is done.
It may be that formula needs to be changed.
    - {{more correct, as we don't completely know yet and need to explore this more}}
- It is important, so let's change the feeding formulas right away to target
babies born earlier!
    - {{almost correct, but this is also too hasty}}

### Exercise: Unreliability of p-value, importance of estimate

{{NE, expanding on previous MCQ}}

- NE: Example of how small changes to data can influence p-value.

### Exercise: Post-processing of model results

{{NE: to show post log transforming}}

# Notes:

- Statistics (more of a review, expect them to know what they are doing):
    - Logistic regression
    - Mixed effects modelling
- Choice of statistic is dependent on question asked.
- In general, cohorts try to address questions such as:
    - "what type of exposures increase the risk of disease?"
    - "how much and how long do individuals need to be exposured to a risk factor
    to develop the disease?"
    - "For those that have a disease, how do they differ from those without?"
    - "Those that have more exposure over time, are they more likely to develop a disease?"
- These questions generally require some type of regression modelling in order to
estimate magnitude of association and the uncertainty around that association.
